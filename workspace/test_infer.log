W1219 15:53:34.638000 2300508 site-packages/torch/distributed/run.py:774] 
W1219 15:53:34.638000 2300508 site-packages/torch/distributed/run.py:774] *****************************************
W1219 15:53:34.638000 2300508 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1219 15:53:34.638000 2300508 site-packages/torch/distributed/run.py:774] *****************************************
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.module.decoder_layer.dense_decoder_layer.DenseDecoderLayer.forward with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.tensor_to_per_block_fp8_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.cast_to_per_block_fp8_with_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.triton_kernels.per_block_quant_gemm.per_block_quant_torch with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.cast_to_per_tensor_fp8_with_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.float8_linear_tensor_wise.per_tensor_fp8_quant with options: {'fullgraph': True}
[FSDP Sharding]:   0%|          | 0/36 [00:00<?, ?it/s]/mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/model/utils/checkpointing.py:92: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return ptd_checkpoint_wrapper(module, *args, **kwargs)
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.module.decoder_layer.dense_decoder_layer.DenseDecoderLayer.forward with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.tensor_to_per_block_fp8_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.cast_to_per_block_fp8_with_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.triton_kernels.per_block_quant_gemm.per_block_quant_torch with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.fsdp_utils.cast_to_per_tensor_fp8_with_scales with options: {'fullgraph': True}
[XTuner][2025-12-19 15:53:41][INFO] Enabling torch.compile for function xtuner.v1.float8.float8_linear_tensor_wise.per_tensor_fp8_quant with options: {'fullgraph': True}
[FSDP Sharding]:   0%|          | 0/36 [00:00<?, ?it/s]/mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/model/utils/checkpointing.py:92: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return ptd_checkpoint_wrapper(module, *args, **kwargs)
[FSDP Sharding]: 100%|██████████| 36/36 [00:00<00:00, 401.52it/s]
[FSDP Sharding]: 100%|██████████| 36/36 [00:00<00:00, 394.51it/s]
[XTuner][2025-12-19 15:53:48][SUCCESS] [HF loading cost] Elapsed time 6.82 seconds, peak gpu memory 17.0G
[XTuner][2025-12-19 15:53:48][SUCCESS] [HF loading cost] Elapsed time 6.81 seconds, peak gpu memory 17.0G
[rank1]:[W1219 15:53:51.792320414 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.

!!! ATTENTION !!!

Type 'up' to get to the frame that called dist.breakpoint(rank=0)

> /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/distributed/__init__.py(111)breakpoint()
-> meta_in_tls = torch._C._meta_in_tls_dispatch_include()
(Pdb) *** Invalid frame count (.)
(Pdb) > /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/fx/interpreter.py(175)run()
-> torch.distributed.breakpoint()
(Pdb) linear
(Pdb) *** NameError: name 'noe' is not defined
(Pdb) ['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_args', '_erased', '_input_nodes', '_kwargs', '_next', '_pretty_print_target', '_prev', '_remove_from_list', '_rename', '_repr_fn', '_sort_key', '_update_args_kwargs', 'all_input_nodes', 'append', 'args', 'format_node', 'graph', 'insert_arg', 'is_impure', 'kwargs', 'meta', 'name', 'next', 'normalized_arguments', 'op', 'prepend', 'prev', 'replace_all_uses_with', 'replace_input_with', 'stack_trace', 'target', 'type', 'update_arg', 'update_kwarg', 'users']
(Pdb) '%linear : [num_users=1] = call_function[target=torch._C._nn.linear](args = (%hidden_states, %l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_, None), kwargs = {})'
(Pdb) <torch.fx.graph.Graph object at 0x7fd5e805ac00>
(Pdb) GraphModule()
(Pdb) 'class GraphModule(torch.nn.Module):\n    def forward(self, l_args_0_: "bf16[1, 14, 4096]", l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_args_1_0_: "bf16[1, 14, 128]", l_args_1_1_: "bf16[1, 14, 128]", l_args_3_0_0_: "bf16[2, 256, 8, 128]", l_args_3_0_1_: "bf16[2, 256, 8, 128]", l_args_2_cu_seq_lens_k: "i32[2]", l_args_2_max_length_q: "i64[]", l_args_2_max_length_k: "i64[]", l_args_2_block_table: "i32[1, 1]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_: "bf16[12288, 4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_: "bf16[12288, 4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 12288]"):\n        # No stacktrace found for following nodes\n        _enter_inference_mode = torch.autograd.grad_mode._enter_inference_mode(True)\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)\n        hidden_states: "bf16[1, 14, 4096]" = torch.rms_norm(l_args_0_, (4096,), l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_, 1e-06);  l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        linear: "bf16[1, 14, 4096]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:204 in forward_prefilling, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        view: "bf16[1, 14, 32, 128]" = linear.view((1, 14, -1, 128));  linear = None\n        query_states: "bf16[1, 32, 14, 128]" = view.transpose(1, 2);  view = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        linear_1: "bf16[1, 14, 1024]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:205 in forward_prefilling, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        view_1: "bf16[1, 14, 8, 128]" = linear_1.view((1, 14, -1, 128));  linear_1 = None\n        key_states: "bf16[1, 8, 14, 128]" = view_1.transpose(1, 2);  view_1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        linear_2: "bf16[1, 14, 1024]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:206 in forward_prefilling, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        view_2: "bf16[1, 14, 8, 128]" = linear_2.view((1, 14, -1, 128));  linear_2 = None\n        value_states: "bf16[1, 8, 14, 128]" = view_2.transpose(1, 2);  view_2 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)\n        query_states_1: "bf16[1, 32, 14, 128]" = torch.rms_norm(query_states, (128,), l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_, 1e-06);  query_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)\n        key_states_1: "bf16[1, 8, 14, 128]" = torch.rms_norm(key_states, (128,), l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_, 1e-06);  key_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:45 in apply_rotary_pos_emb_cuda, code: cos = cos.unsqueeze(unsqueeze_dim)\n        cos: "bf16[1, 1, 14, 128]" = l_args_1_0_.unsqueeze(1);  l_args_1_0_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:46 in apply_rotary_pos_emb_cuda, code: sin = sin.unsqueeze(unsqueeze_dim)\n        sin: "bf16[1, 1, 14, 128]" = l_args_1_1_.unsqueeze(1);  l_args_1_1_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:47 in apply_rotary_pos_emb_cuda, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n        mul: "bf16[1, 32, 14, 128]" = query_states_1 * cos\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:13 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]\n        x1: "bf16[1, 32, 14, 64]" = query_states_1[(Ellipsis, slice(None, 64, None))]\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:14 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]\n        x2: "bf16[1, 32, 14, 64]" = query_states_1[(Ellipsis, slice(64, None, None))];  query_states_1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:15 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)\n        neg: "bf16[1, 32, 14, 64]" = -x2;  x2 = None\n        cat: "bf16[1, 32, 14, 128]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:47 in apply_rotary_pos_emb_cuda, code: q_embed = (q * cos) + (rotate_half(q) * sin)\n        mul_1: "bf16[1, 32, 14, 128]" = cat * sin;  cat = None\n        q_embed: "bf16[1, 32, 14, 128]" = mul + mul_1;  mul = mul_1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:48 in apply_rotary_pos_emb_cuda, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n        mul_2: "bf16[1, 8, 14, 128]" = key_states_1 * cos;  cos = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:13 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]\n        x1_1: "bf16[1, 8, 14, 64]" = key_states_1[(Ellipsis, slice(None, 64, None))]\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:14 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]\n        x2_1: "bf16[1, 8, 14, 64]" = key_states_1[(Ellipsis, slice(64, None, None))];  key_states_1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:15 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)\n        neg_1: "bf16[1, 8, 14, 64]" = -x2_1;  x2_1 = None\n        cat_1: "bf16[1, 8, 14, 128]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:48 in apply_rotary_pos_emb_cuda, code: k_embed = (k * cos) + (rotate_half(k) * sin)\n        mul_3: "bf16[1, 8, 14, 128]" = cat_1 * sin;  cat_1 = sin = None\n        k_embed: "bf16[1, 8, 14, 128]" = mul_2 + mul_3;  mul_2 = mul_3 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/_library/custom_ops.py:681 in __call__, code: return self._opoverload(*args, **kwargs)\n        fill_paged_kv_cache_default = torch.ops.xtuner.fill_paged_kv_cache.default(k_embed, value_states, l_args_3_0_0_, l_args_3_0_1_, l_args_2_cu_seq_lens_k, l_args_2_cu_seq_lens_k, l_args_2_max_length_q, l_args_2_max_length_k, l_args_2_block_table);  l_args_3_0_0_ = l_args_3_0_1_ = l_args_2_block_table = fill_paged_kv_cache_default = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:236 in forward_prefilling, code: query_states.transpose(1, 2).squeeze(0),\n        transpose_3: "bf16[1, 14, 32, 128]" = q_embed.transpose(1, 2);  q_embed = None\n        squeeze: "bf16[14, 32, 128]" = transpose_3.squeeze(0);  transpose_3 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:237 in forward_prefilling, code: key_states.transpose(1, 2).squeeze(0),\n        transpose_4: "bf16[1, 14, 8, 128]" = k_embed.transpose(1, 2);  k_embed = None\n        squeeze_1: "bf16[14, 8, 128]" = transpose_4.squeeze(0);  transpose_4 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:238 in forward_prefilling, code: value_states.transpose(1, 2).squeeze(0),\n        transpose_5: "bf16[1, 14, 8, 128]" = value_states.transpose(1, 2);  value_states = None\n        squeeze_2: "bf16[14, 8, 128]" = transpose_5.squeeze(0);  transpose_5 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/_library/custom_ops.py:681 in __call__, code: return self._opoverload(*args, **kwargs)\n        _flash_attn_varlen_forward_v2_default = torch.ops.flash_attn._flash_attn_varlen_forward_v2.default(squeeze, squeeze_1, squeeze_2, l_args_2_cu_seq_lens_k, l_args_2_cu_seq_lens_k, l_args_2_max_length_q, l_args_2_max_length_k, 0.0, 0.08838834764831845, causal = True, window_size_left = -1, window_size_right = -1, softcap = 0.0, alibi_slopes = None, return_softmax = False, block_table = None);  squeeze = squeeze_1 = squeeze_2 = l_args_2_cu_seq_lens_k = l_args_2_max_length_q = l_args_2_max_length_k = None\n        out_padded: "bf16[14, 32, 128]" = _flash_attn_varlen_forward_v2_default[0]\n        softmax_lse: "f32[32, 14]" = _flash_attn_varlen_forward_v2_default[1];  softmax_lse = None\n        S_dmask: "bf16[0]" = _flash_attn_varlen_forward_v2_default[2];  S_dmask = None\n        rng_state: "i64[2]" = _flash_attn_varlen_forward_v2_default[3];  _flash_attn_varlen_forward_v2_default = rng_state = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/flash_attn/gpu.py:438 in forward, code: out = out_padded[..., :head_size_og]\n        out: "bf16[14, 32, 128]" = out_padded[(Ellipsis, slice(None, 128, None))];  out_padded = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:248 in forward_prefilling, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n        reshape: "bf16[1, 14, 4096]" = out.reshape(1, 14, -1);  out = None\n        attn_output: "bf16[1, 14, 4096]" = reshape.contiguous();  reshape = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        projected_output: "bf16[1, 14, 4096]" = torch._C._nn.linear(attn_output, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:96 in forward, code: hidden_states = residual + hidden_states\n        hidden_states_1: "bf16[1, 14, 4096]" = l_args_0_ + projected_output;  l_args_0_ = projected_output = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)\n        hidden_states_2: "bf16[1, 14, 4096]" = torch.rms_norm(hidden_states_1, (4096,), l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_, 1e-06);  l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        linear_4: "bf16[1, 14, 12288]" = torch._C._nn.linear(hidden_states_2, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/act_fn.py:46 in native_silu, code: return F.silu(x)\n        silu: "bf16[1, 14, 12288]" = torch.nn.functional.silu(linear_4);  linear_4 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        linear_5: "bf16[1, 14, 12288]" = torch._C._nn.linear(hidden_states_2, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:35 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n        mul_4: "bf16[1, 14, 12288]" = silu * linear_5;  silu = linear_5 = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)\n        down_proj: "bf16[1, 14, 4096]" = torch._C._nn.linear(mul_4, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_ = None\n        \n         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:102 in forward, code: hidden_states = residual + hidden_states\n        hidden_states_3: "bf16[1, 14, 4096]" = hidden_states_1 + down_proj;  hidden_states_1 = down_proj = None\n        \n        # No stacktrace found for following nodes\n        _exit_inference_mode = torch.autograd.grad_mode._exit_inference_mode(_enter_inference_mode);  _enter_inference_mode = _exit_inference_mode = None\n        return (hidden_states_3,)\n        '
(Pdb) class GraphModule(torch.nn.Module):
    def forward(self, l_args_0_: "bf16[1, 14, 4096]", l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_args_1_0_: "bf16[1, 14, 128]", l_args_1_1_: "bf16[1, 14, 128]", l_args_3_0_0_: "bf16[2, 256, 8, 128]", l_args_3_0_1_: "bf16[2, 256, 8, 128]", l_args_2_cu_seq_lens_k: "i32[2]", l_args_2_max_length_q: "i64[]", l_args_2_max_length_k: "i64[]", l_args_2_block_table: "i32[1, 1]", l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_: "bf16[12288, 4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_: "bf16[12288, 4096]", l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 12288]"):
        # No stacktrace found for following nodes
        _enter_inference_mode = torch.autograd.grad_mode._enter_inference_mode(True)
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        hidden_states: "bf16[1, 14, 4096]" = torch.rms_norm(l_args_0_, (4096,), l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_, 1e-06);  l_self_modules_checkpoint_wrapped_module_modules_input_layernorm_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        linear: "bf16[1, 14, 4096]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:204 in forward_prefilling, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        view: "bf16[1, 14, 32, 128]" = linear.view((1, 14, -1, 128));  linear = None
        query_states: "bf16[1, 32, 14, 128]" = view.transpose(1, 2);  view = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        linear_1: "bf16[1, 14, 1024]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:205 in forward_prefilling, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        view_1: "bf16[1, 14, 8, 128]" = linear_1.view((1, 14, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 14, 128]" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        linear_2: "bf16[1, 14, 1024]" = torch._C._nn.linear(hidden_states, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_v_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:206 in forward_prefilling, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        view_2: "bf16[1, 14, 8, 128]" = linear_2.view((1, 14, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 14, 128]" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        query_states_1: "bf16[1, 32, 14, 128]" = torch.rms_norm(query_states, (128,), l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_, 1e-06);  query_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_q_norm_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        key_states_1: "bf16[1, 8, 14, 128]" = torch.rms_norm(key_states, (128,), l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_, 1e-06);  key_states = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_k_norm_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:45 in apply_rotary_pos_emb_cuda, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 14, 128]" = l_args_1_0_.unsqueeze(1);  l_args_1_0_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:46 in apply_rotary_pos_emb_cuda, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 14, 128]" = l_args_1_1_.unsqueeze(1);  l_args_1_1_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:47 in apply_rotary_pos_emb_cuda, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul: "bf16[1, 32, 14, 128]" = query_states_1 * cos
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:13 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 14, 64]" = query_states_1[(Ellipsis, slice(None, 64, None))]
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:14 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 14, 64]" = query_states_1[(Ellipsis, slice(64, None, None))];  query_states_1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:15 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 14, 64]" = -x2;  x2 = None
        cat: "bf16[1, 32, 14, 128]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:47 in apply_rotary_pos_emb_cuda, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1: "bf16[1, 32, 14, 128]" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 14, 128]" = mul + mul_1;  mul = mul_1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:48 in apply_rotary_pos_emb_cuda, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2: "bf16[1, 8, 14, 128]" = key_states_1 * cos;  cos = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:13 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 14, 64]" = key_states_1[(Ellipsis, slice(None, 64, None))]
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:14 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 14, 64]" = key_states_1[(Ellipsis, slice(64, None, None))];  key_states_1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:15 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 14, 64]" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 14, 128]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/rotary_emb.py:48 in apply_rotary_pos_emb_cuda, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3: "bf16[1, 8, 14, 128]" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 14, 128]" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/_library/custom_ops.py:681 in __call__, code: return self._opoverload(*args, **kwargs)
        fill_paged_kv_cache_default = torch.ops.xtuner.fill_paged_kv_cache.default(k_embed, value_states, l_args_3_0_0_, l_args_3_0_1_, l_args_2_cu_seq_lens_k, l_args_2_cu_seq_lens_k, l_args_2_max_length_q, l_args_2_max_length_k, l_args_2_block_table);  l_args_3_0_0_ = l_args_3_0_1_ = l_args_2_block_table = fill_paged_kv_cache_default = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:236 in forward_prefilling, code: query_states.transpose(1, 2).squeeze(0),
        transpose_3: "bf16[1, 14, 32, 128]" = q_embed.transpose(1, 2);  q_embed = None
        squeeze: "bf16[14, 32, 128]" = transpose_3.squeeze(0);  transpose_3 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:237 in forward_prefilling, code: key_states.transpose(1, 2).squeeze(0),
        transpose_4: "bf16[1, 14, 8, 128]" = k_embed.transpose(1, 2);  k_embed = None
        squeeze_1: "bf16[14, 8, 128]" = transpose_4.squeeze(0);  transpose_4 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:238 in forward_prefilling, code: value_states.transpose(1, 2).squeeze(0),
        transpose_5: "bf16[1, 14, 8, 128]" = value_states.transpose(1, 2);  value_states = None
        squeeze_2: "bf16[14, 8, 128]" = transpose_5.squeeze(0);  transpose_5 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/_library/custom_ops.py:681 in __call__, code: return self._opoverload(*args, **kwargs)
        _flash_attn_varlen_forward_v2_default = torch.ops.flash_attn._flash_attn_varlen_forward_v2.default(squeeze, squeeze_1, squeeze_2, l_args_2_cu_seq_lens_k, l_args_2_cu_seq_lens_k, l_args_2_max_length_q, l_args_2_max_length_k, 0.0, 0.08838834764831845, causal = True, window_size_left = -1, window_size_right = -1, softcap = 0.0, alibi_slopes = None, return_softmax = False, block_table = None);  squeeze = squeeze_1 = squeeze_2 = l_args_2_cu_seq_lens_k = l_args_2_max_length_q = l_args_2_max_length_k = None
        out_padded: "bf16[14, 32, 128]" = _flash_attn_varlen_forward_v2_default[0]
        softmax_lse: "f32[32, 14]" = _flash_attn_varlen_forward_v2_default[1];  softmax_lse = None
        S_dmask: "bf16[0]" = _flash_attn_varlen_forward_v2_default[2];  S_dmask = None
        rng_state: "i64[2]" = _flash_attn_varlen_forward_v2_default[3];  _flash_attn_varlen_forward_v2_default = rng_state = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/flash_attn/gpu.py:438 in forward, code: out = out_padded[..., :head_size_og]
        out: "bf16[14, 32, 128]" = out_padded[(Ellipsis, slice(None, 128, None))];  out_padded = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/attention/mha.py:248 in forward_prefilling, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape: "bf16[1, 14, 4096]" = out.reshape(1, 14, -1);  out = None
        attn_output: "bf16[1, 14, 4096]" = reshape.contiguous();  reshape = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        projected_output: "bf16[1, 14, 4096]" = torch._C._nn.linear(attn_output, l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output = l_self_modules_checkpoint_wrapped_module_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:96 in forward, code: hidden_states = residual + hidden_states
        hidden_states_1: "bf16[1, 14, 4096]" = l_args_0_ + projected_output;  l_args_0_ = projected_output = None
        
         # File: /mnt/shared-storage-user/chenchiyu/miniconda3/envs/py12/lib/python3.12/site-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        hidden_states_2: "bf16[1, 14, 4096]" = torch.rms_norm(hidden_states_1, (4096,), l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_, 1e-06);  l_self_modules_checkpoint_wrapped_module_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        linear_4: "bf16[1, 14, 12288]" = torch._C._nn.linear(hidden_states_2, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_gate_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/ops/act_fn.py:46 in native_silu, code: return F.silu(x)
        silu: "bf16[1, 14, 12288]" = torch.nn.functional.silu(linear_4);  linear_4 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        linear_5: "bf16[1, 14, 12288]" = torch._C._nn.linear(hidden_states_2, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_up_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:35 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        mul_4: "bf16[1, 14, 12288]" = silu * linear_5;  silu = linear_5 = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/linear/linear.py:24 in forward, code: return F.linear(input, w, b)
        down_proj: "bf16[1, 14, 4096]" = torch._C._nn.linear(mul_4, l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_checkpoint_wrapped_module_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /mnt/shared-storage-user/chenchiyu/gitcode/xtuner/xtuner/v1/module/decoder_layer/dense_decoder_layer.py:102 in forward, code: hidden_states = residual + hidden_states
        hidden_states_3: "bf16[1, 14, 4096]" = hidden_states_1 + down_proj;  hidden_states_1 = down_proj = None
        
        # No stacktrace found for following nodes
        _exit_inference_mode = torch.autograd.grad_mode._exit_inference_mode(_enter_inference_mode);  _enter_inference_mode = _exit_inference_mode = None
        return (hidden_states_3,)
        
(Pdb) 170  	                continue
171  	
172  	            try:
173  	                self.env[node] = self.run_node(node)
174  	            except Exception as e:
175  ->	                torch.distributed.breakpoint()
176  	                if self.extra_traceback:
177  	                    msg = f"While executing {node.format_node()}"
178  	                    msg = f"{e.args[0]}\n\n{msg}" if e.args else str(msg)
179  	                    if (
180  	                        isinstance(self.module, GraphModule)
(Pdb) *** KeyError: <weakref at 0x7fd1bb8dea20; to 'torch.storage.UntypedStorage' at 0x7fd1bb8def80>
(Pdb) [rank1]:[W1219 16:03:51.857519159 socket.cpp:460] [c10d] waitForInput: poll for socket SocketImpl(fd=45, addr=[localhost]:45556, remote=[localhost]:29500) returned 0, likely a timeout
[rank1]:[W1219 16:03:51.857679226 socket.cpp:485] [c10d] waitForInput: socket SocketImpl(fd=45, addr=[localhost]:45556, remote=[localhost]:29500) timed out after 600000ms
